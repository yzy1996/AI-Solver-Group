@article{chen2025pangu,
  title={Pangu Light: Weight Re-Initialization for Pruning and Accelerating LLMs},
  author={Chen, Hanting and Qin, Jiarui and Guo, Jialong and Yuan, Tao and Yin, Yichun and Zhen, Huiling and Wang, Yasheng and Li, Jinpeng and Meng, Xiaojun and Zhang, Meng and others},
  journal={arXiv preprint arXiv:2505.20155},
  year={2025}
} 

@article{li2025kvtuner,
  title={KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache Quantization for Efficient and Nearly Lossless LLM Inference},
  author={Li, Xing and Xing, Zeyu and Li, Yiming and Qu, Linping and Zhen, Hui-Ling and Liu, Wulong and Yao, Yiwu and Pan, Sinno Jialin and Yuan, Mingxuan},
  journal={ICML2025},
  year={2025}
}

@article{wang2025accelerating,
  title={Accelerating Large Language Model Reasoning via Speculative Search},
  author={Wang, Zhihai and Wang, Jie and Pan, Jilai and Xia, Xilin and Zhen, Huiling and Yuan, Mingxuan and Hao, Jianye and Wu, Feng},
  journal={ICML2025},
  year={2025}
}

@article{gui2025hypertree,
  title={HyperTree Planning: Enhancing LLM Reasoning via Hierarchical Thinking},
  author={Gui, Runquan and Wang, Zhihai and Wang, Jie and Ma, Chi and Zhen, Huiling and Yuan, Mingxuan and Hao, Jianye and Lian, Defu and Chen, Enhong and Wu, Feng},
  journal={ICML2025},
  year={2025}
}

@article{he2025certifying,
  title={Certifying Language Model Robustness with Fuzzed Randomized Smoothing: An Efficient Defense Against Backdoor Attacks},
  author={He, Bowei and Yin, Lihao and Zhen, Hui-Ling and Zhang, Jianping and Hong, Lanqing and Yuan, Mingxuan and Ma, Chen},
  journal={ICLR2025},
  year={2025}
}

@article{pei2024fusegpt,
  title={FuseGPT: Learnable Layers Fusion of Generative Pre-trained Transformers},
  author={Pei, Zehua and Zhen, Hui-Ling and Yu, Xianzhi and Pan, Sinno Jialin and Yuan, Mingxuan and Yu, Bei},
  journal={arXiv preprint arXiv:2411.14507},
  year={2024}
}


@article{pei2025premoe,
  title={PreMoe: Lightening MoEs on Constrained Memory by Expert Pruning and Retrieval},
  author={Pei, Zehua and Zhang, Ying and Zhen, Hui-Ling and Yu, Xianzhi and Liu, Wulong and Pan, Sinno Jialin and Yuan, Mingxuan and Yu, Bei},
  journal={arXiv preprint arXiv:2505.17639},
  year={2025}
}

@article{lin2025trimr,
  title={TrimR: Verifier-based Training-Free Thinking Compression for Efficient Test-Time Scaling},
  author={Lin, Weizhe and Li, Xing and Yang, Zhiyuan and Fu, Xiaojin and Zhen, Hui-Ling and Wang, Yaoyuan and Yu, Xianzhi and Liu, Wulong and Li, Xiaosong and Yuan, Mingxuan},
  journal={arXiv preprint arXiv:2505.17155},
  year={2025}
}

@article{yankun2025svdq,
  title={SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention},
  author={Yankun, Hong and Xing, Li and Hui-Ling, Zhen and Xianzhi, Yu and Wulong, Liu and Mingxuan, Yuan},
  journal={arXiv preprint arXiv:2502.15304},
  year={2025}
}

@article{he2025paser,
  title={PASER: Post-Training Data Selection for Efficient Pruned Large Language Model Recovery},
  author={He, Bowei and Yin, Lihao and Zhen, Hui-Ling and Zhang, Xiaokun and Yuan, Mingxuan and Ma, Chen},
  journal={arXiv preprint arXiv:2502.12594},
  year={2025}
}


@article{pei2025cmoe,
  title={CMoE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference},
  author={Pei, Zehua and Zou, Lancheng and Zhen, Hui-Ling and Yu, Xianzhi and Liu, Wulong and Pan, Sinno Jialin and Yuan, Mingxuan and Yu, Bei},
  journal={arXiv preprint arXiv:2502.04416},
  year={2025}
}




